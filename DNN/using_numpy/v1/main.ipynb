{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from header import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_of_data_train = 1000\n",
    "num_of_data_test = int(num_of_data_train * 0.2)\n",
    "#\n",
    "num_of_features = 30\n",
    "#\n",
    "x_train = np.random.randn(num_of_data_train, num_of_features)\n",
    "x_test = np.random.randn(num_of_data_test, num_of_features)\n",
    "y_train = np.zeros(shape=(num_of_data_train, 1))\n",
    "y_test = np.zeros(shape=(num_of_data_test, 1))\n",
    "#\n",
    "for m in range(num_of_data_train):\n",
    "    if m > 0.5 * num_of_data_train:\n",
    "        y_train[m] = 1\n",
    "\n",
    "np.random.shuffle(x_train)\n",
    "np.random.shuffle(y_train)\n",
    "#\n",
    "for m in range(num_of_data_test):\n",
    "    if m > 0.5 * num_of_data_test:\n",
    "        y_test[m] = 1\n",
    "#\n",
    "np.random.shuffle(x_test)\n",
    "np.random.shuffle(y_test)\n",
    "#\n",
    "layers = [num_of_features, 20, 16, 1]\n",
    "num_of_layers = len(layers)\n",
    "#\n",
    "alpha = 0.001\n",
    "lamb = 0.001\n",
    "#\n",
    "params = {}\n",
    "for i in range(1, num_of_layers):\n",
    "    params[\"w\" + str(i)] = np.random.rand(layers[i - 1], layers[i]) * 0.001\n",
    "    params[\"b\" + str(i)] = np.zeros((1, layers[i]))\n",
    "\n",
    "\n",
    "def grad_loss(a, y, m, loss_function):\n",
    "    if loss_function == 'cross_entropy':\n",
    "        return - (y / a) / m\n",
    "    elif loss_function == 'MSE':\n",
    "        return (a - y) / m\n",
    "\n",
    "\n",
    "def calc_loss(y, a, m, loss_function):\n",
    "    if loss_function == \"cross_entropy\":\n",
    "        return np.sum(-(y * np.log(a))) / m\n",
    "    elif loss_function == \"MSE\":\n",
    "        return np.sum(np.square(y - a)) / (2 * m)\n",
    "\n",
    "\n",
    "\n",
    "grads = {}\n",
    "a = {}\n",
    "z = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 100\n",
    "    num_of_iterations = 1000\n",
    "    activation_func = 'sigmoid'\n",
    "    loss_func = 'cross_entropy'\n",
    "    total_loss = []\n",
    "    for i in range(epochs):\n",
    "        for j in range(num_of_iterations):\n",
    "            x = x_train\n",
    "            for ilayer in range(1, num_of_layers):\n",
    "                w = params[\"w\" + str(ilayer)]\n",
    "                b = params[\"b\" + str(ilayer)]\n",
    "                z[ilayer] = np.matmul(x, w) + b\n",
    "                if ilayer == num_of_layers - 1:\n",
    "                    a[ilayer] = activation_function(z[ilayer], 'sigmoid')\n",
    "                else:\n",
    "                    a[ilayer] = activation_function(z[ilayer], activation_func)\n",
    "                x = a[ilayer]\n",
    "                print('forward',i,j,ilayer)\n",
    "\n",
    "            for ilayer in range(num_of_layers - 1, 0, -1):\n",
    "                print('backward',i,j,ilayer)\n",
    "                if ilayer == num_of_layers - 1:\n",
    "                    dLda = grad_loss(a[ilayer], y_train, num_of_data_train, loss_func)\n",
    "                    print('dLda is', dLda)\n",
    "\n",
    "                    dadz = grad_actiavtion_function(z[ilayer], 'softmax')\n",
    "                    print('dadz is', dadz)\n",
    "\n",
    "                    #temp = linear_grad(dLda, dadz)\n",
    "                    temp = np.matmul(dadz.transpose(),dLda)\n",
    "                    print('temp is', temp)\n",
    "                    #temp = dLda * dadz  # element by element operation\n",
    "                    dzdw = a[ilayer - 1]\n",
    "                    grads[\"dw\" + str(ilayer)] = np.matmul(dzdw.transpose(), temp)\n",
    "                    print(grads[\"dw\" + str(ilayer)])\n",
    "                    grads[\"db\" + str(ilayer)] = np.mean(temp)\n",
    "                    print(grads[\"db\" + str(ilayer)])\n",
    "                else:\n",
    "                    dzda = params[\"w\" + str(ilayer+1)]\n",
    "                    dadz = grad_actiavtion_function(z[ilayer], activation_func)\n",
    "                    temp = np.matmul(temp, dzda.transpose())\n",
    "                    temp = temp * dadz\n",
    "                    if ilayer == 1:\n",
    "                        dzdw = x_train\n",
    "                    else:\n",
    "                        dzdw = a[ilayer-1]\n",
    "                    grads[\"dw\" + str(ilayer)] = np.matmul(dzdw.transpose(), temp)\n",
    "                    grads[\"db\" + str(ilayer)] = np.mean(temp)\n",
    "            for ilayer in range(1, num_of_layers):\n",
    "                params[\"w\" + str(ilayer)] = params[\"w\" + str(ilayer)] - alpha * grads[\"dw\" + str(ilayer)]\n",
    "                params[\"b\" + str(ilayer)] = params[\"b\" + str(ilayer)] - alpha * grads[\"db\" + str(ilayer)]\n",
    "\n",
    "            if j % 100 == 0:\n",
    "                loss = calc_loss(y_train, a[num_of_layers-1], num_of_data_train, loss_func)\n",
    "                total_loss.append(loss)\n",
    "                print(f\"epoch: {i}, iteration: {j} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    plt.figure()\n",
    "    plt.plot(total_loss)\n",
    "    plt.xlabel(\"num of iterations\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"DNN\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}